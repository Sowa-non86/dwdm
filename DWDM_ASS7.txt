import pandas as pd
from mlxtend.frequent_patterns import fpgrowth, association_rules
from mlxtend.preprocessing import TransactionEncoder

# Step 1: Load the dataset
def load_data(file_path):
    data = pd.read_csv(file_path)
    return data

# Step 2: Clean and preprocess data
def preprocess_data(file_path):
    data = load_data(file_path)
    
    # Convert non-numeric 'na' or other non-numeric entries in 'ping' and 'kb/s' to NaN
    data['ping'] = pd.to_numeric(data['ping'], errors='coerce')
    data['kb/s'] = pd.to_numeric(data['kb/s'], errors='coerce')
    
    # Fill missing values with the median of respective columns
    data.fillna({'ping': data['ping'].median(), 'kb/s': data['kb/s'].median()}, inplace=True)
    
    # Apply one-hot encoding for categorical columns
    categorical_columns = ['operator', 'network', 'type', 'Circle', 'month']
    data = pd.get_dummies(data, columns=categorical_columns)
    
    # Discretize the 'kb/s' and 'ping' columns into categories
    data['kb/s'] = pd.cut(data['kb/s'], bins=[0, 500, 1000, float('inf')], labels=['Low_Speed', 'Medium_Speed', 'High_Speed'])
    data['ping'] = pd.cut(data['ping'], bins=[0, 50, 100, float('inf')], labels=['Good_Ping', 'Medium_Ping', 'Poor_Ping'])
    
    # Convert all data to a format suitable for association rule mining
    return data

# Step 3: Apply FP-Growth algorithm and generate rules
def apply_fpgrowth(data, min_support=0.1, max_len=3):
    te = TransactionEncoder()
    transformed_data = te.fit_transform(data.astype(str).values)
    df = pd.DataFrame(transformed_data, columns=te.columns_)
    
    frequent_itemsets = fpgrowth(df, min_support=min_support, max_len=max_len, use_colnames=True)
    return frequent_itemsets

# Step 4: Generate association rules
def generate_rules(frequent_itemsets, min_confidence=0.5, min_lift=1.0):
    rules = association_rules(frequent_itemsets, metric="lift", min_threshold=min_lift)
    rules = rules[rules['confidence'] >= min_confidence]
    return rules

# Step 5: Interpret the results
def interpret_rules(rules):
    print("Discovered Rules with High Confidence and Lift:")
    for index, rule in rules.iterrows():
        print(f"Rule {index+1}:")
        print(f"  IF {set(rule['antecedents'])} THEN {set(rule['consequents'])}")
        print(f"  Support: {rule['support']:.4f}")
        print(f"  Confidence: {rule['confidence']:.4f}")
        print(f"  Lift: {rule['lift']:.4f}")
        print("-" * 50)

# Step 6: Main function to execute the process
def main(file_path):
    # Preprocess data
    cleaned_data = preprocess_data(file_path)
    
    # Apply FP-Growth to find frequent itemsets
    frequent_itemsets = apply_fpgrowth(cleaned_data)
    
    # Generate association rules
    rules = generate_rules(frequent_itemsets)
    
    # Interpret the discovered rules
    interpret_rules(rules)

# Example usage
file_path = 'D:\PROGRAMMING\PYTHON\DWDM\Indian-Internet-dataset.csv'  # Replace with the correct file path
main(file_path)
